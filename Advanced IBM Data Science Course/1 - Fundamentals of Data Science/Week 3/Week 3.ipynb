{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd36338-a107-4125-b8c6-4381347b9c60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b14b9f6c-73f0-4544-82f1-ea6f6be86056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pyspark==3.1.2 -q\n",
    "!pip install findspark -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d92d13d8-2d13-45e1-be99-07a36898ad28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6e6a299-87d0-48a0-a5ea-38e0e0d00bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError as e:\n",
    "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09e170c-4405-41d7-90f4-60da7b878a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 17:12:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1961a8ec-769b-49d6-8466-9793fe77e8af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78dc0740-dc92-4ad8-afab-564085a2712b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,4,5,34,1,32,4,34,2,1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49ee45e-c399-4cf5-8c03-faacdc015887",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum = rdd.sum()\n",
    "n = rdd.count()\n",
    "mean = sum/n\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d224b8-0dca-4075-a1ac-715358d1c7f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the median\n",
    "\n",
    "# This value is middle number of a range of numbers. Out-lier resistant rather than the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb335eb5-d161-4905-b53b-52ba8344d360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"MedianExample\").getOrCreate()\n",
    "\n",
    "# Replace 'your_list' with your actual list of numbers\n",
    "your_list = [1,2,4,5,34,1,32,4,34,2,1,3]\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "df = spark.createDataFrame([(value,) for value in your_list], [\"values\"])\n",
    "\n",
    "# Calculate the median\n",
    "n = df.count()\n",
    "sorted_df = df.sort(\"values\")\n",
    "median = sorted_df.selectExpr(\"percentile(values, 0.5)\").collect()[0][0]\n",
    "\n",
    "print(\"Median:\", median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f1c4fc-c0a8-4738-8636-ce3294bd1216",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2rd Staistical moment.\n",
    "## Standard Deviation\n",
    "### Represents how wide the data is spread around the mean.\n",
    "\n",
    "# The histogram represents data frequency distribution. \n",
    "### _/\\_ Standard deviation is low. \n",
    "### _/    \\_ Standard deviation is higher. \n",
    "\n",
    "# The variance is the standar deviation to the power of two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6940362d-79a5-4e35-9556-110a8121e9a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "# Standard deviation\n",
    "sqrt(rdd.map(lambda x : pow(x-mean, 2)).sum()/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8273a3b-5557-4d0e-80fa-be2d2ced4d63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the standard deviation\n",
    "rdd = sc.parallelize([34,1,23,4,3,3,12,4,3,1])\n",
    "\n",
    "n = rdd.count()\n",
    "std_dev = sqrt(rdd.map(lambda x: pow(x - mean, 2)).sum() / n)\n",
    "\n",
    "print(\"Standard Deviation:\", std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bbdc56-9a75-4880-ac19-be2dcfe55416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3rd Staistical moment.\n",
    "# Skewness\n",
    "# How asymmetric data is spread round the mean. \n",
    "\n",
    "# If the tail is on the left hand side, we talk about negative skew.\n",
    "# ong tailed, thin left skew, and a short tailed, thick right skew, \n",
    "# they might cancel out if you obtain a value close to zero for the skewness measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a72d081-e543-474a-bf3f-3f2719feb348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate skewness\n",
    "\n",
    "rdd = sc.parallelize([34,1,23,4,3,3,12,4,3,1])\n",
    "\n",
    "# Calculate the mean\n",
    "mean = rdd.mean()\n",
    "n = float(n)\n",
    "skewness = (1/n) * rdd.map(lambda x: pow(x - mean, 3) / pow(sd, 3)).sum()\n",
    "\n",
    "print(\"Skewness:\", skewness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8abcded-6dfb-4fb5-aa7e-a27bc2dc2ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4rd Staistical moment.\n",
    "# Kurtosis\n",
    "\n",
    "# Reports on the shape of the data\n",
    "# Indicates outlier content within the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e367755-7fff-490e-81f3-069ba5a8888e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create your RDD\n",
    "rdd = sc.parallelize([1, 2, 4, 5, 3, 4, 1, 3, 2, 4, 3, 4, 2, 1, 3])\n",
    "\n",
    "# Calculate the mean\n",
    "mean = rdd.mean()\n",
    "\n",
    "# Calculate the standard deviation\n",
    "n = rdd.count()\n",
    "sd = sqrt(rdd.map(lambda x: pow(x - mean, 2)).sum() / n)\n",
    "\n",
    "# Calculate kurtosis\n",
    "n = float(n)\n",
    "kurtosis = rdd.map(lambda x: pow(x - mean, 4) / pow(sd, 4)).sum() / n\n",
    "\n",
    "print(\"Kurtosis:\", kurtosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1e637f-a851-4e20-81ef-e2b67b2936c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Theoric Example\n",
    "#Question\n",
    "#Consider an IoT system measuring fuel consumption of cars.\n",
    "#The idea is to detect engine anomalies or potential failures using the collected\n",
    "#fuel consumption of all cars of a specific type. If fuel consumption exceeds \n",
    "#a certain threshold an alert should be generated. How can kurtosis be used \n",
    "#to find a good threshold value? \n",
    "\n",
    "\n",
    "\n",
    "#Solution\n",
    "#Kurtosis is a measure of outlier content. \n",
    "#The more outliers are present in examples of fuel consumption \n",
    "#where engines didn't fail, the higher \n",
    "#the false positive rate gets when choosing a threshold value which is too low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e6542a3-98a4-429f-a90d-18cf76288379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Covariance and correlation\n",
    "# Covariance and correlation tell us how two columns are interacting with each other.\n",
    "\n",
    "rddX = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "rddY = sc.parallelize([7,6,5,4,5,6,7,8,9,10])\n",
    "\n",
    "meanX = rddX.sum()/rddX.count()\n",
    "meanY = rddY.sum()/rddY.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43953954-e726-40d1-a414-b4fc50354aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rddXY = rddX.zip(rddY)\n",
    "covXY = rddXY.map(lambda xy: (xy[0] - meanX) * (xy[1] - meanY)).sum() / rddXY.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f587b17-9fc5-40fe-934d-4bb625ccf801",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.65"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Covarianza\n",
    "covXY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f461d90-95c7-45e3-a203-e0090ca92783",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Statistics' has no attribute 'covariance'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2420/3724244787.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Calculate covariance matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcov_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Extract the covariance value between rddX and rddY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Statistics' has no attribute 'covariance'"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "# Combine rddX and rddY into an RDD of paired elements\n",
    "data = rddX.zip(rddY)\n",
    "\n",
    "# Calculate covariance matrix\n",
    "cov_matrix = Statistics.covariance(data)\n",
    "\n",
    "# Extract the covariance value between rddX and rddY\n",
    "cov_value = cov_matrix[0, 1]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "cor_matrix = Statistics.corr(data)\n",
    "\n",
    "# Extract the correlation value between rddX and rddY\n",
    "cor_value = cor_matrix[0, 1]\n",
    "\n",
    "print(\"Covariance: {}\".format(cov_value))\n",
    "print(\"Correlation: {}\".format(cor_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed97edf-521a-4f38-80a1-ad24100a9f12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Measure of depndency - Correlation\n",
    "\n",
    "# +1 Colums totally correlate\n",
    "# 0 No interaction at all\n",
    "# -1 Inverse dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a9c00-e0aa-4b6c-b54b-3e4892bacde3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import sqrt \n",
    "\n",
    "n = rddXY.count()\n",
    "sdX = sqrt(rddX.map(lambda x : pow(x-mean, 2)).sum()/n)\n",
    "sdX = sqrt(rddX.map(lambda x : pow(x-mean, 2)).sum()/n)\n",
    "print(sdX)\n",
    "print(sdY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99230d19-ffe5-465e-9be3-40997286b3f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corrXY = covXY / (sdX * sdY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa3cd74-dd53-4b39-88b9-cbc6d33382b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27279178-c84d-4bd8-81f9-53631dc71d38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1240558842.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_2420/1240558842.py\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    data = column1.zip(column2).zip(column3).zip(column4).map((((a,b),c),c), d : (a, b,c,d))\u001b[0m\n\u001b[0m                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pyspark.mllib.stat import Staistics\n",
    "\n",
    "column1 = sc.parallelize(range(100))\n",
    "column2 = sc.parallelize(range(100, 200))\n",
    "column3 = sc.parallelize(list(reversed(ranged(100))))\n",
    "column4 = sc.parallelize(random.smaple(range(100), 100))\n",
    "# Concat different columns\n",
    "data = column1.zip(column2).zip(column3).zip(column4).map((((a,b),c),c), d : (a, b,c,d))\n",
    "Staistics.corr(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0edf9e4-a90c-4963-a3a3-894e279faad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multidimensional vector spaces.\n",
    "# dimensions or features instead of columns and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba7653-d121-4fee-aed4-ae0ca5299df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 dimensions\n",
    "# Point clouds -> Clusters\n",
    "# The process of agruping the data in clusters is called algorithmic clustering.\n",
    "# What about aditional dimensions?\n",
    "\n",
    "# If low cadinality we canc olor code dat points or use different shapes\n",
    "\n",
    "#Plot inmost releveant dimensions\n",
    "#Create multiple plots with sibsits.\n",
    "# Reduce dimensions using dimensionality reduction.\n",
    "# Use clustering and clasification algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bee35f-af60-4b44-ab2e-d309b2c09fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f8dd7-63cb-42a2-b37a-ff90c46715c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4e2059-de14-4c2e-9058-c472725839f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40206d4-5414-4b2a-8578-97e190d98037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a191826f-0f9b-4435-ac34-591fd0017a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8633a3a7-8d66-483b-b66f-9e87ccec1ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de89a8-9a6a-4531-a018-4563a538c924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f0f458-309b-4898-b14b-c79bbcd11019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71b11be-7fb2-40d2-a36f-9c660e0ef4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1d97c-92dc-4b37-860b-9716826894f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72961c6d-c3da-4adb-99a5-fca4233c49e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
